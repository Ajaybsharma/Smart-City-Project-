{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart City System Analysis\n",
    "\n",
    "This notebook implements a comprehensive Smart City system focusing on sustainability analysis and optimization. The system collects, processes, and analyzes urban data to optimize resource usage and enhance sustainability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Union, Optional, Tuple\n",
    "import logging\n",
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection Layer\n",
    "\n",
    "The data collection layer handles gathering data from various sources including OpenStreetMap, real estate datasets, and IoT sensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class OpenStreetMapSource:\n",
    "    \"\"\"Data source for OpenStreetMap API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_endpoint=\"https://nominatim.openstreetmap.org/search\"):\n",
    "        self.api_endpoint = api_endpoint\n",
    "    \n",
    "    def fetch_data(self, query, limit=10):\n",
    "        \"\"\"Fetch geographical data from OpenStreetMap.\"\"\"\n",
    "        try:\n",
    "            params = {\n",
    "                'q': query,\n",
    "                'format': 'json',\n",
    "                'limit': limit\n",
    "            }\n",
    "            \n",
    "            response = requests.get(self.api_endpoint, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            return pd.DataFrame(data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "class RealEstateDataSource:\n",
    "    \"\"\"Data source for real estate datasets.\"\"\"\n",
    "    \n",
    "    def fetch_data(self):\n",
    "        \"\"\"Generate sample real estate data.\"\"\"\n",
    "        return pd.DataFrame({\n",
    "            'property_id': range(1, 11),\n",
    "            'price': [250000, 300000, 350000, 400000, 450000, 500000, 550000, 600000, 650000, 700000],\n",
    "            'size_sqft': [1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600, 2800],\n",
    "            'bedrooms': [2, 2, 3, 3, 3, 4, 4, 4, 5, 5],\n",
    "            'location': ['Downtown', 'Suburb', 'Downtown', 'Suburb', 'Downtown', 'Suburb', 'Downtown', 'Suburb', 'Downtown', 'Suburb']\n",
    "        })\n",
    "\n",
    "class IoTEnvironmentalSource:\n",
    "    \"\"\"Data source for IoT environmental sensors.\"\"\"\n",
    "    \n",
    "    def fetch_data(self):\n",
    "        \"\"\"Generate sample IoT sensor data.\"\"\"\n",
    "        return pd.DataFrame({\n",
    "            'sensor_id': ['S001', 'S002', 'S003', 'S004', 'S005'],\n",
    "            'temperature': [22.5, 23.1, 21.8, 24.2, 22.9],\n",
    "            'humidity': [45, 48, 52, 40, 47],\n",
    "            'air_quality_index': [65, 70, 85, 60, 75],\n",
    "            'water_usage': [100, 120, 90, 110, 105],\n",
    "            'timestamp': pd.date_range(start='2023-01-01', periods=5, freq='H')\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Layer\n",
    "\n",
    "The data processing layer handles data cleaning, feature engineering, and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"Cleans and prepares raw data for analysis.\"\"\"\n",
    "    \n",
    "    def preprocess(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply preprocessing steps to data.\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "            \n",
    "        # Remove duplicates\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "        # Handle missing values\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        categorical_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            df[col] = df[col].fillna(df[col].mean())\n",
    "            \n",
    "        for col in categorical_cols:\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "        \n",
    "        return df\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Creates meaningful features from raw data.\"\"\"\n",
    "    \n",
    "    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Generate new features.\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "            \n",
    "        result_df = df.copy()\n",
    "        \n",
    "        # Create time-based features if timestamp exists\n",
    "        if 'timestamp' in df.columns:\n",
    "            result_df['hour'] = df['timestamp'].dt.hour\n",
    "            result_df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "            result_df['month'] = df['timestamp'].dt.month\n",
    "            \n",
    "        # Create interaction features for numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for i, col1 in enumerate(numeric_cols):\n",
    "            for col2 in numeric_cols[i+1:]:\n",
    "                result_df[f\"{col1}_times_{col2}\"] = df[col1] * df[col2]\n",
    "        \n",
    "        return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Model Layer\n",
    "\n",
    "The AI model layer handles training and evaluation of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Trains machine learning models on processed data.\"\"\"\n",
    "    \n",
    "    def train(self, X: pd.DataFrame, y: pd.Series, model_type: str = \"random_forest\") -> Dict[str, Any]:\n",
    "        \"\"\"Train a machine learning model.\"\"\"\n",
    "        try:\n",
    "            # Split data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Train model\n",
    "            model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            train_score = model.score(X_train_scaled, y_train)\n",
    "            test_score = model.score(X_test_scaled, y_test)\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"model\": model,\n",
    "                \"scaler\": scaler,\n",
    "                \"train_score\": train_score,\n",
    "                \"test_score\": test_score\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training model: {e}\")\n",
    "            return {\"status\": \"error\", \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sustainability Metrics Layer\n",
    "\n",
    "The sustainability metrics layer analyzes various environmental factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class WaterMetricsAnalyzer:\n",
    "    \"\"\"Analyzes water usage patterns.\"\"\"\n",
    "    \n",
    "    def analyze_consumption(self, data: pd.DataFrame, consumption_column: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze water consumption patterns.\"\"\"\n",
    "        if data.empty:\n",
    "            return {\"status\": \"error\", \"message\": \"Empty dataset\"}\n",
    "            \n",
    "        total_consumption = data[consumption_column].sum()\n",
    "        avg_consumption = data[consumption_column].mean()\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"total_consumption\": float(total_consumption),\n",
    "            \"average_consumption\": float(avg_consumption),\n",
    "            \"stats\": {\n",
    "                \"min\": float(data[consumption_column].min()),\n",
    "                \"max\": float(data[consumption_column].max()),\n",
    "                \"std\": float(data[consumption_column].std())\n",
    "            }\n",
    "        }\n",
    "\n",
    "class EnvironmentalFactorsAnalyzer:\n",
    "    \"\"\"Analyzes environmental parameters.\"\"\"\n",
    "    \n",
    "    def analyze_air_quality(self, data: pd.DataFrame, aqi_column: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze air quality data.\"\"\"\n",
    "        if data.empty:\n",
    "            return {\"status\": \"error\", \"message\": \"Empty dataset\"}\n",
    "            \n",
    "        avg_aqi = data[aqi_column].mean()\n",
    "        \n",
    "        # Determine air quality category\n",
    "        if avg_aqi <= 50:\n",
    "            category = \"Good\"\n",
    "        elif avg_aqi <= 100:\n",
    "            category = \"Moderate\"\n",
    "        elif avg_aqi <= 150:\n",
    "            category = \"Unhealthy for Sensitive Groups\"\n",
    "        else:\n",
    "            category = \"Unhealthy\"\n",
    "            \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"average_aqi\": float(avg_aqi),\n",
    "            \"category\": category,\n",
    "            \"stats\": {\n",
    "                \"min\": float(data[aqi_column].min()),\n",
    "                \"max\": float(data[aqi_column].max()),\n",
    "                \"std\": float(data[aqi_column].std())\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Layer\n",
    "\n",
    "The optimization layer applies machine learning to optimize sustainability metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class AIOptimizer:\n",
    "    \"\"\"Applies machine learning for optimization.\"\"\"\n",
    "    \n",
    "    def optimize(self, data: pd.DataFrame, target_column: str, \n",
    "                feature_columns: List[str], constraints: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Optimize target variable based on constraints.\"\"\"\n",
    "        try:\n",
    "            # Train model\n",
    "            X = data[feature_columns]\n",
    "            y = data[target_column]\n",
    "            \n",
    "            trainer = ModelTrainer()\n",
    "            model_result = trainer.train(X, y)\n",
    "            \n",
    "            if model_result[\"status\"] != \"success\":\n",
    "                return model_result\n",
    "            \n",
    "            model = model_result[\"model\"]\n",
    "            scaler = model_result[\"scaler\"]\n",
    "            \n",
    "            # Generate random samples within constraints\n",
    "            num_samples = 1000\n",
    "            samples = {}\n",
    "            \n",
    "            for feature in feature_columns:\n",
    "                if feature in constraints:\n",
    "                    min_val = constraints[feature].get(\"min\", data[feature].min())\n",
    "                    max_val = constraints[feature].get(\"max\", data[feature].max())\n",
    "                    samples[feature] = np.random.uniform(min_val, max_val, num_samples)\n",
    "                else:\n",
    "                    samples[feature] = np.random.uniform(data[feature].min(), data[feature].max(), num_samples)\n",
    "            \n",
    "            samples_df = pd.DataFrame(samples)\n",
    "            samples_scaled = scaler.transform(samples_df)\n",
    "            \n",
    "            # Make predictions\n",
    "            predictions = model.predict(samples_scaled)\n",
    "            \n",
    "            # Find optimal solution\n",
    "            best_idx = np.argmin(predictions)  # Assuming we want to minimize the target\n",
    "            \n",
    "            return {\n",
    "                \"status\": \"success\",\n",
    "                \"optimal_values\": dict(samples_df.iloc[best_idx]),\n",
    "                \"predicted_target\": float(predictions[best_idx])\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during optimization: {e}\")\n",
    "            return {\"status\": \"error\", \"message\": str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Let's demonstrate the system with some example data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize components\n",
    "iot_source = IoTEnvironmentalSource()\n",
    "preprocessor = DataPreprocessor()\n",
    "feature_engineer = FeatureEngineer()\n",
    "water_analyzer = WaterMetricsAnalyzer()\n",
    "env_analyzer = EnvironmentalFactorsAnalyzer()\n",
    "optimizer = AIOptimizer()\n",
    "\n",
    "# Collect data\n",
    "iot_data = iot_source.fetch_data()\n",
    "print(\"\\nCollected IoT data:\")\n",
    "display(iot_data.head())\n",
    "\n",
    "# Preprocess data\n",
    "processed_data = preprocessor.preprocess(iot_data)\n",
    "featured_data = feature_engineer.create_features(processed_data)\n",
    "print(\"\\nProcessed data with engineered features:\")\n",
    "display(featured_data.head())\n",
    "\n",
    "# Analyze water consumption\n",
    "water_analysis = water_analyzer.analyze_consumption(featured_data, 'water_usage')\n",
    "print(\"\\nWater consumption analysis:\")\n",
    "print(json.dumps(water_analysis, indent=2))\n",
    "\n",
    "# Analyze air quality\n",
    "air_analysis = env_analyzer.analyze_air_quality(featured_data, 'air_quality_index')\n",
    "print(\"\\nAir quality analysis:\")\n",
    "print(json.dumps(air_analysis, indent=2))\n",
    "\n",
    "# Optimize water usage\n",
    "optimization_result = optimizer.optimize(\n",
    "    featured_data,\n",
    "    target_column='water_usage',\n",
    "    feature_columns=['temperature', 'humidity', 'hour'],\n",
    "    constraints={\n",
    "        'temperature': {'min': 20, 'max': 25},\n",
    "        'humidity': {'min': 40, 'max': 60}\n",
    "    }\n",
    ")\n",
    "print(\"\\nOptimization results:\")\n",
    "print(json.dumps(optimization_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's create some visualizations of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set up the plotting style\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# Create a figure with multiple subplots\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Water usage over time\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(featured_data['timestamp'], featured_data['water_usage'], marker='o')\n",
    "plt.title('Water Usage Over Time')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Water Usage')\n",
    "\n",
    "# 2. Air Quality Index distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(featured_data['air_quality_index'], bins=10)\n",
    "plt.title('Air Quality Index Distribution')\n",
    "plt.xlabel('AQI')\n",
    "\n",
    "# 3. Temperature vs Water Usage\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(featured_data['temperature'], featured_data['water_usage'])\n",
    "plt.title('Temperature vs Water Usage')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Water Usage')\n",
    "\n",
    "# 4. Humidity vs Air Quality\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(featured_data['humidity'], featured_data['air_quality_index'])\n",
    "plt.title('Humidity vs Air Quality')\n",
    "plt.xlabel('Humidity')\n",
    "plt.ylabel('Air Quality Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}